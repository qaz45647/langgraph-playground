{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98660640",
   "metadata": {},
   "source": [
    "# SFT 監督式微調"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf006826",
   "metadata": {},
   "source": [
    "## 什麼是 SFT (Supervised Fine-Tuning)?\n",
    "\n",
    "Supervised Fine-Tuning (SFT) 是最直接的微調方法,使用標註好的「輸入-輸出」對來訓練模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21865428",
   "metadata": {},
   "source": [
    "## SFT 的優勢與限制\n",
    "**優勢:**\n",
    "- 概念簡單,易於理解和實現\n",
    "- 資料需求相對明確\n",
    "- 訓練穩定,收斂快\n",
    "- 可以快速適應特定任務\n",
    "\n",
    "**限制:**\n",
    "- 需要大量高質量標註資料\n",
    "- 可能過擬合訓練資料\n",
    "- 對資料質量極為敏感"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445800e",
   "metadata": {},
   "source": [
    "## 資料格式設計\n",
    "\n",
    "Alpaca 格式（單輪對話）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae467e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"instruction\": \"將以下英文翻譯成中文\",\n",
    "  \"input\": \"The weather is beautiful today.\",\n",
    "  \"output\": \"今天天氣很好。\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f92dcf",
   "metadata": {},
   "source": [
    "Chat 格式（多輪對話）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fbd795",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"conversations\": [\n",
    "    {\"role\": \"user\", \"content\": \"什麼是機器學習？\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"機器學習是人工智慧的一個分支...\"},\n",
    "    {\"role\": \"user\", \"content\": \"能舉個例子嗎？\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"當然！例如垃圾郵件過濾...\"}\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e1731",
   "metadata": {},
   "source": [
    "實際使用時的 Prompt 模板："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c7e29",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "<|system|>你是一個有幫助的 AI 助手</s>\n",
    "<|user|>什麼是機器學習？</s>\n",
    "<|assistant|>機器學習是人工智慧的一個分支...</s>\n",
    "<|user|>能舉個例子嗎？</s>\n",
    "<|assistant|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02586bf4",
   "metadata": {},
   "source": [
    "模型學習在 <|assistant|> 標記後生成回應。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7f654",
   "metadata": {},
   "source": [
    "## 訓練流程與實作重點\n",
    "\n",
    "### Step 1：載入模型與 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475588f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# LLaMA 類模型訓練必關\n",
    "model.config.use_cache = False\n",
    "\n",
    "# 設定 padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0553ff8",
   "metadata": {},
   "source": [
    "- 使用 HuggingFace 的 LLaMA 2 7B\n",
    "    - 需同意 LLaMA 2 license\n",
    "    - 並登入 HuggingFace（huggingface-cli login）\n",
    "- LLaMA 預設會開 KV cache，訓練時會造成錯誤或VRAM暴增，所以一般使用者記得關掉"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c09e7f",
   "metadata": {},
   "source": [
    "### Step 2：載入並格式化資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530505e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"train_data.json\")\n",
    "\n",
    "def format_prompt(example):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Input:\n",
    "{example['input']}\n",
    "\n",
    "### Response:\n",
    "{example['output']}\"\"\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "dataset = dataset.map(format_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34929dc7",
   "metadata": {},
   "source": [
    "**train_data.json 格式長這樣：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "  {\n",
    "    \"instruction\": \"請解釋什麼是 Transformer\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Transformer 是一種神經網路架構...\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598f09d",
   "metadata": {},
   "source": [
    "### Step 3：Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9b3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,   # 超過長度直接切\n",
    "        max_length=512,\n",
    "        padding=False,  # 交給 data collator 動態 padding\n",
    "    )\n",
    "    # Causal LM：labels = input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"instruction\", \"input\", \"output\", \"text\"],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47923d9",
   "metadata": {},
   "source": [
    "### Step 4：訓練參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sft_output\",\n",
    "    num_train_epochs=3,  # 跑 3 輪資料\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # 有效 batch size = 16\n",
    "    learning_rate=2e-5,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,  # 每 100 step 存一次\n",
    "    save_total_limit=2,  # 最多留 2 個 checkpoint\n",
    "    warmup_steps=100,   # 防止一開始炸掉\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f08c2",
   "metadata": {},
   "source": [
    "**Batch Size：**\n",
    "- 小模型（< 7B）：16-32\n",
    "- 模型（7B-70B）：4-8\n",
    "\n",
    "**Learning Rate：**\n",
    "- SFT 通常用較小的 LR：1e-5 到 5e-5\n",
    "- 比預訓練的 LR（約 3e-4）小 10 倍\n",
    "- 原因：避免破壞預訓練學到的知識\n",
    "\n",
    "**LR Scheduler：**\n",
    "- warmup_ratio = 0.03  # 前 3% 步驟 warmup\n",
    "- lr_scheduler = \"cosine\"  # 餘弦衰減\n",
    "\n",
    "**Epoch 數量：**\n",
    "- 通常2-5 epochs就足夠\n",
    "- 過多 epoch 容易 overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9bac10",
   "metadata": {},
   "source": [
    "### Step 5：Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd20d1",
   "metadata": {},
   "source": [
    "### Step 6：開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb09fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e76235",
   "metadata": {},
   "source": [
    "### Step 7：儲存最終模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd074100",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./sft_final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be02fa",
   "metadata": {},
   "source": [
    "### 完整程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971de8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# =========================\n",
    "# 1. 載入模型與 tokenizer\n",
    "# =========================\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# LLaMA 類模型訓練必關\n",
    "model.config.use_cache = False\n",
    "\n",
    "# 設定 padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# =========================\n",
    "# 2. 載入並格式化資料\n",
    "# =========================\n",
    "dataset = load_dataset(\"json\", data_files=\"train_data.json\")\n",
    "\n",
    "def format_prompt(example):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Input:\n",
    "{example['input']}\n",
    "\n",
    "### Response:\n",
    "{example['output']}\"\"\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "# =========================\n",
    "# 3. Tokenization\n",
    "# =========================\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,  # 交給 data collator 動態 padding\n",
    "    )\n",
    "    # Causal LM：labels = input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"instruction\", \"input\", \"output\", \"text\"],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 4. 訓練參數\n",
    "# =========================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sft_output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # 有效 batch size = 16\n",
    "    learning_rate=2e-5,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=100,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 5. Trainer\n",
    "# =========================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 6. 開始訓練\n",
    "# =========================\n",
    "trainer.train()\n",
    "\n",
    "# =========================\n",
    "# 8. 儲存模型\n",
    "# =========================\n",
    "trainer.save_model(\"./sft_final_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f40d7dd",
   "metadata": {},
   "source": [
    "## 訓練最佳化\n",
    "### 超參數調優（Ray Tune）\n",
    "\n",
    "自動搜尋 learning rate、batch size 等參數，找 eval loss 最小 的組合。\n",
    "\n",
    "**核心概念：**\n",
    "- objective：一次完整訓練 + 回報指標\n",
    "- search space：定義要嘗試的超參數範圍\n",
    "- ASHAScheduler：提早中止表現差的實驗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aef409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.air import session\n",
    "\n",
    "def objective(config):\n",
    "    \"\"\"訓練並回報評估指標\"\"\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        per_device_train_batch_size=config[\"batch_size\"],\n",
    "        num_train_epochs=config[\"num_epochs\"],\n",
    "        warmup_ratio=config[\"warmup_ratio\"],\n",
    "\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        callbacks=[MonitorCallback()],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    session.report({\n",
    "        \"loss\": eval_results[\"eval_loss\"]\n",
    "    })\n",
    "\n",
    "# 設定搜尋空間\n",
    "search_space = {\n",
    "    \"learning_rate\": tune.loguniform(1e-5, 1e-3),\n",
    "    \"batch_size\": tune.choice([2, 4, 8]),\n",
    "    \"num_epochs\": tune.choice([2, 3, 5]),\n",
    "    \"warmup_ratio\": tune.uniform(0.0, 0.2)\n",
    "}\n",
    "\n",
    "# 執行超參數搜尋\n",
    "analysis = tune.run(\n",
    "    objective,\n",
    "    config=search_space,\n",
    "    num_samples=20,\n",
    "    scheduler=ASHAScheduler(metric=\"loss\", mode=\"min\"),\n",
    "    resources_per_trial={\n",
    "        \"cpu\": 4,\n",
    "        \"gpu\": 1,  # 教學假設 1 GPU\n",
    "    }\n",
    ")\n",
    "\n",
    "best_config = analysis.best_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf8530",
   "metadata": {},
   "source": [
    "### 持續監控與改進\n",
    "**即時發現：**\n",
    "- loss 突然暴增\n",
    "- 梯度爆炸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726514c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor:\n",
    "    \"\"\"訓練過程監控\"\"\"\n",
    "    def __init__(self):\n",
    "        self.metrics_history = []\n",
    "        self.alert_thresholds = {\n",
    "            \"loss_spike\": 2.0,  # loss 突然增加\n",
    "            \"gradient_norm\": 10.0   # 梯度爆炸\n",
    "        }\n",
    "\n",
    "    def log_step(self, step, metrics):\n",
    "        \"\"\"記錄每個訓練步驟\"\"\"\n",
    "        if metrics is None:\n",
    "            return\n",
    "\n",
    "        self.metrics_history.append({\n",
    "            \"step\": step,\n",
    "            \"timestamp\": time.time(),\n",
    "            **metrics\n",
    "        })\n",
    "\n",
    "        self.check_alerts(metrics)\n",
    "\n",
    "    def check_alerts(self, metrics):\n",
    "        \"\"\"檢查異常情況\"\"\"\n",
    "        if len(self.metrics_history) < 2:\n",
    "            return\n",
    "\n",
    "        prev_loss = self.metrics_history[-2].get(\"loss\")\n",
    "        curr_loss = metrics.get(\"loss\")\n",
    "\n",
    "        # 檢測 loss 異常\n",
    "        if prev_loss is not None and curr_loss is not None:\n",
    "            if curr_loss > prev_loss * self.alert_thresholds[\"loss_spike\"]:\n",
    "                print(f\"Loss spike: {prev_loss:.4f} → {curr_loss:.4f}\")\n",
    "                # 可以自動降低學習率或回滾\n",
    "        \n",
    "        # 檢測梯度異常\n",
    "        if metrics.get(\"grad_norm\", 0) > self.alert_thresholds[\"gradient_norm\"]:\n",
    "            print(f\"Gradient explosion: {metrics['grad_norm']:.2f}\")\n",
    "\n",
    "# 使用監控器\n",
    "monitor = TrainingMonitor()\n",
    "\n",
    "# 在 Trainer callback 中使用\n",
    "class MonitorCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            monitor.log_step(state.global_step, logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b73a8",
   "metadata": {},
   "source": [
    "## 常見問題排查\n",
    "\n",
    "### Overfitting（過擬合）：\n",
    "- 模型過度記憶訓練資料，失去泛化能力\n",
    "- 症狀：\n",
    "    - 訓練 loss 持續下降，驗證 loss 上升\n",
    "    - 對訓練樣本回答完美，但對新問題回答變差\n",
    "    - 開始重複訓練資料的確切用詞\n",
    "- 預防方法：\n",
    "    1. Early stopping\n",
    "    2. Dropout\n",
    "    3. weight decay\n",
    "    4. 增加資料多樣性\n",
    "    5. 降低學習率或減少 epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3901295",
   "metadata": {},
   "source": [
    "\n",
    "### Catastrophic Forgetting（災難性遺忘）：\n",
    "- 微調過程中，模型忘記預訓練學到的知識\n",
    "- 症狀：\n",
    "    - 原本能回答的常識問題變得不會\n",
    "    - 特定領域能力突然下降（如數學推理）\n",
    "\n",
    "- 預防方法：\n",
    "    1. 混入預訓練資料：10-20% 的預訓練資料\n",
    "    2. 使用較小學習率：保留原有權重\n",
    "    3. 使用 LoRA 等 PEFT 方法\n",
    "    4. 限制訓練步驟：不要過度訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac83322",
   "metadata": {},
   "source": [
    "### OOM（Out Of Memory，記憶體不足）：\n",
    "- GPU / RAM 不足，導致訓練或推理失敗\n",
    "- 症狀：\n",
    "    - CUDA out of memory\n",
    "    - 訓練中途崩潰\n",
    "    - 輸入稍長就無法推理\n",
    "- 預防方法：\n",
    "    - 減小 batch size\n",
    "    - 縮短序列長度\n",
    "    - 使用 FP16 / BF16\n",
    "    - Gradient accumulation / checkpointing\n",
    "    - 使用 LoRA 等 PEFT 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee8c647",
   "metadata": {},
   "source": [
    "## SFT 的限制與問題\n",
    "### 幻覺問題（Hallucination）\n",
    "\n",
    "SFT 模型可能會自信地生成錯誤資訊：\n",
    "```\n",
    "使用者：台灣最高的山是哪座？\n",
    "模型：台灣最高的山是雪山，海拔3886公尺。\n",
    "     （錯誤，正確答案是玉山，3952公尺）\n",
    "```\n",
    "**原因**：\n",
    "- SFT 只學習「模仿訓練資料的風格」，不保證事實正確性\n",
    "- 模型傾向生成流暢、自信的回應，即使不確定\n",
    "- 訓練資料本身可能包含錯誤\n",
    "\n",
    "**緩解方法**（但無法完全解決）：\n",
    "- 提高訓練資料品質\n",
    "- 加入「不確定」的回答示範\n",
    "- 結合檢索增強（RAG）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66373c3c",
   "metadata": {},
   "source": [
    "### 人類偏好未被明確建模\n",
    "\n",
    "SFT 假設「訓練資料中的回應都是好的」，但現實更複雜：\n",
    "```\n",
    "問題：推薦一部電影\n",
    "\n",
    "回應A：我推薦《刺激1995》，這是一部經典的劇情片...\n",
    "回應B：我推薦《刺激1995》，這是 IMDB 評分最高的電影，\n",
    "        講述獄中友誼與希望的故事，結局催淚...\n",
    "```\n",
    "\n",
    "兩個回應都「正確」，但 B 明顯更好（更詳細、更有用）。SFT 無法區分這種品質差異。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1310d6",
   "metadata": {},
   "source": [
    "### 無法比較「好 vs 更好」\n",
    "\n",
    "SFT 的目標函數是「最大化正確回應的機率」，而非「讓好的回應機率 > 差的回應機率」。\n",
    "\n",
    "**舉例**：\n",
    "```\n",
    "問題：如何快速入睡？\n",
    "\n",
    "差的回應：閉上眼睛就能睡著\n",
    "好的回應：可以嘗試以下方法：1) 睡前一小時避免螢幕 \n",
    "          2) 保持房間涼爽黑暗 3) 練習深呼吸...\n",
    "```\n",
    "SFT 只要兩個回應都出現在訓練資料中，就會同等看待。它不會學到「好的回應更值得生成」。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf2d3c6",
   "metadata": {},
   "source": [
    "後續會介紹的 DPO 和 PPO——它們可以透過比較不同回應的好壞，讓模型學會人類偏好。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
