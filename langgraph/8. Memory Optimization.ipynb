{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "121df493",
   "metadata": {},
   "source": [
    "# LangGraph 中的記憶體最佳化與管理\n",
    "## 為什麼需要記憶體管理?\n",
    "\n",
    "當我們建構具備記憶能力的 Agent 時，最容易忽略的就是記憶體管理的重要性。首先是 LLM 的上下文限制問題，不同模型有不同的 token 上限，例如：\n",
    "- GPT-4 從 8k 到 128k tokens\n",
    "- Claude 達 200k tokens\n",
    "\n",
    "但問題在於：Agent 是持續運行的。\n",
    "當對話與內部狀態不斷累積，很快就會觸及模型的 token 天花板，導致：\n",
    "- 對話被強制截斷\n",
    "- 模型注意力被耗散\n",
    "- token 消耗大\n",
    "- 效能降低\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534992b",
   "metadata": {},
   "source": [
    "## 記憶類型與生命週期概覽\n",
    "\n",
    "要有效管理 Agent 的記憶，第一步是理解：\n",
    "不是所有記憶都應該被同樣對待。\n",
    "\n",
    "我們可以依「用途」與「生命週期」將記憶劃分為不同類型。\n",
    "### 1. 短期對話記憶（Short-term Conversation Memory）\n",
    "- 用途：維持當前對話的上下文連貫性\n",
    "- 生命週期：僅限於單一會話\n",
    "- 常見策略：\n",
    "    - 只保留最近幾輪\n",
    "    - 或最近 N 個 tokens\n",
    "\n",
    "這類記憶不需要永久保存，過期即丟是正常設計。\n",
    "\n",
    "### 2. 任務與目標記憶（Task / Goal Memory）\n",
    "- 記錄內容：\n",
    "    - 當前任務目標\n",
    "    - 已完成步驟\n",
    "    - 待辦事項（TODO）\n",
    "\n",
    "- 特性：與特定任務綁定\n",
    "\n",
    "當任務完成後，這類記憶可以：\n",
    "- 被歸檔\n",
    "- 或直接清除\n",
    "\n",
    "不需要長期佔用「工作記憶」。\n",
    "\n",
    "### 3. 長期使用者與知識記憶（Long-term Memory）\n",
    "這一類包括：\n",
    "- 使用者偏好\n",
    "- 累積的背景知識\n",
    "- 系統運行經驗\n",
    "\n",
    "理論上它們需要長期保存，但實務上仍必須引入衰減機制，因為：\n",
    "\n",
    "- 偏好會改變\n",
    "- 知識會過時\n",
    "- 長期未使用的記憶，價值會下降\n",
    "\n",
    "「永久保存」不等於「永久活躍」。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f2abb1",
   "metadata": {},
   "source": [
    "## Active Memory 與 Archived Memory 的分層設計\n",
    "\n",
    "在實際系統中，一個常見且有效的做法是將記憶分成兩個層級。\n",
    "### 1. Active Memory（主動記憶）\n",
    "Active Memory 是系統中的熱資料（Hot Data）層,存放那些頻繁被存取、對當前任務至關重要的記憶。這類記憶通常包括最近的對話歷史、正在進行的任務狀態,以及使用者最常用到的偏好設定。由於需要在每次 Agent 推理時快速讀取，Active Memory 會保存在高效能的儲存系統中，例如記憶體資料庫或向量資料庫的高速索引區。然而，高效能意味著高成本，因此 Active Memory 的容量必須受到嚴格控制，通常只保留數千到數萬條記錄。系統會透過重要性評分、存取頻率和時效性等指標，持續評估哪些記憶應該留在這一層，一旦記憶的價值下降或長時間未被使用，就會被降級到 Archived Memory，為更重要的新記憶騰出空間。\n",
    "\n",
    "### Archived Memory（封存記憶）\n",
    "Archived Memory 是系統的冷資料（Cold Data）層，用於長期保存那些不常使用但未來可能還需要的記憶。這些記憶可能是幾週前的對話內容、已完成任務的歷史記錄，或是很少被觸發的使用者偏好。由於存取頻率低，Archived Memory 可以使用成本較低的儲存方案，例如物件儲存或壓縮後的資料庫，檢索速度雖然較慢但容量幾乎不受限制。重要的是,歸檔並不等於刪除,當 Agent 在 Active Memory 中找不到相關資訊時，仍然可以向 Archived Memory 發起查詢，如果發現某條歸檔記憶突然變得重要(例如使用者提到很久以前的話題)，系統可以將其重新啟動並提升回 Active Memory。這種動態的分層機制讓 Agent 既能保持快速反應，又能在需要時調用深層的歷史記憶，就像人類大腦在日常思考時主要依賴短期記憶，但在特定情境下也能喚醒塵封已久的往事。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0ee470",
   "metadata": {},
   "source": [
    "## 對話上下文壓縮與裁切\n",
    "### Trimming：限制對話歷史長度\n",
    "\n",
    "**固定窗口：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d52a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_window_trim(messages, max_count=10):\n",
    "    \"\"\"保留最近 N 條訊息\"\"\"\n",
    "    return messages[-max_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9574283",
   "metadata": {},
   "source": [
    "**滑動窗口：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d53770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_trim(messages, max_tokens=4000):\n",
    "    \"\"\"基於 token 數量的滑動窗口\"\"\"\n",
    "    total_tokens = 0\n",
    "    trimmed = []\n",
    "    \n",
    "    for msg in reversed(messages):\n",
    "        msg_tokens = count_tokens(msg)\n",
    "        if total_tokens + msg_tokens > max_tokens:\n",
    "            break\n",
    "        trimmed.insert(0, msg)\n",
    "        total_tokens += msg_tokens\n",
    "    \n",
    "    return trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88f4fa",
   "metadata": {},
   "source": [
    "**在 LangGraph 中的 reducer 實作**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a58a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from typing import Annotated\n",
    "\n",
    "def trim_messages_reducer(existing, new):\n",
    "    \"\"\"自動裁切訊息的 reducer\"\"\"\n",
    "    all_messages = existing + new\n",
    "    return sliding_window_trim(all_messages, max_tokens=4000)\n",
    "\n",
    "class TrimmedState(MessagesState):\n",
    "    messages: Annotated[list, trim_messages_reducer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cff377",
   "metadata": {},
   "source": [
    "### Summarization：壓縮舊對話\n",
    "\n",
    "**摘要時機**\n",
    "\n",
    "- Token 數達閾值(如 80% 上限)\n",
    "- 每 N 輪對話\n",
    "- 任務階段切換時\n",
    "\n",
    "**分段摘要**\n",
    "\n",
    "- 把完整對話 切成固定大小的區塊\n",
    "\n",
    "- 每一段獨立摘要\n",
    "\n",
    "- 最後得到的是「摘要列表（list of summaries）」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b51aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "messages = [m1, m2, m3, ..., m60]\n",
    "   ↓ \n",
    "Segment 1 = [m1  ~ m20]\n",
    "Segment 2 = [m21 ~ m40]\n",
    "Segment 3 = [m41 ~ m60]\n",
    "   ↓ \n",
    "Summary_1 = summarize(Segment 1)\n",
    "Summary_2 = summarize(Segment 2)\n",
    "Summary_3 = summarize(Segment 3)\n",
    "   ↓ \n",
    "summaries = [\n",
    "  Summary_1,\n",
    "  Summary_2,\n",
    "  Summary_3\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df25a8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分段摘要:每段獨立總結\n",
    "def segment_summarize(messages, segment_size=20):\n",
    "    summaries = []\n",
    "    for i in range(0, len(messages), segment_size):\n",
    "        segment = messages[i:i+segment_size]\n",
    "        summary = llm.summarize(segment)\n",
    "        summaries.append(summary)\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4768b86",
   "metadata": {},
   "source": [
    "**累積摘要**\n",
    "\n",
    "- 永遠只有 一份「當前總結」\n",
    "\n",
    "- 每來新對話，就在舊摘要基礎上滾動更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd0432",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Summary₀\n",
    "   ↓ + messages₁\n",
    "Summary₁\n",
    "   ↓ + messages₂\n",
    "Summary₂\n",
    "   ↓ + messages₃\n",
    "Summary₃\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd625836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 累積摘要:滾動更新總結\n",
    "def rolling_summarize(current_summary, new_messages):\n",
    "    prompt = f\"現有摘要:{current_summary}\\n新對話:{new_messages}\\n更新摘要:\"\n",
    "    return llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247ada5",
   "metadata": {},
   "source": [
    "**Summarization Node 設計**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization_node(state: State):\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    if count_tokens(messages) > THRESHOLD:\n",
    "        old_messages = messages[:-10]  # 保留最近 10 條\n",
    "        summary = llm.summarize(old_messages)\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [SystemMessage(content=summary)] + messages[-10:],\n",
    "            \"summary_count\": state.get(\"summary_count\", 0) + 1\n",
    "        }\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db657c7f",
   "metadata": {},
   "source": [
    "## 記憶價值與生命週期管理\n",
    "\n",
    "### 記憶生命週期概念\n",
    "\n",
    "建立 → 使用 → 強化 / 衰減 → 淘汰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de8c62",
   "metadata": {},
   "source": [
    "### 記憶評分與 Metadata 設計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e972fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    content: str\n",
    "    metadata: dict = {\n",
    "        \"importance\": 0.5,      # 記憶的重要性分數 (0~1)\n",
    "        \"recency\": timestamp,   # 最近一次被使用或命中的時間\n",
    "        \"access_count\": 0,      # 被存取或引用的次數\n",
    "        \"created_at\": timestamp,# 建立時間\n",
    "        \"tags\": []              # 主題或語意標籤\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4a11b3",
   "metadata": {},
   "source": [
    "**重要性計算**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_importance(memory):\n",
    "    # 關鍵詞配對\n",
    "    keywords = [\"目標\", \"錯誤\", \"決策\", \"使用者偏好\"]\n",
    "    keyword_score = sum(kw in memory.content for kw in keywords) * 0.2\n",
    "    \n",
    "    # LLM 評估\n",
    "    llm_score = llm.evaluate_importance(memory.content)  # 0-1\n",
    "    \n",
    "    return min(keyword_score + llm_score, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8096deca",
   "metadata": {},
   "source": [
    "### 記憶衰減機制\n",
    "**時間衰減**\n",
    "\n",
    "- 長時間未被使用的記憶，即使曾經重要，也會逐漸失效\n",
    "\n",
    "- 模擬人類對「過時資訊」的自然遺忘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f11972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_decay(memory, decay_rate=0.01):\n",
    "    \"\"\"每天衰減 1%\"\"\"\n",
    "    days_elapsed = (now() - memory.metadata[\"created_at\"]).days\n",
    "    decay_factor = (1 - decay_rate) ** days_elapsed\n",
    "    memory.metadata[\"importance\"] *= decay_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b13ecd9",
   "metadata": {},
   "source": [
    "**使用頻率衰減**\n",
    "\n",
    "- 反覆被使用 = 真正有價值\n",
    "\n",
    "- 防止重要但較舊的記憶被時間衰減過度削弱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb15e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_boost(memory):\n",
    "    \"\"\"存取次數越多越重要\"\"\"\n",
    "    access_count = memory.metadata[\"access_count\"]\n",
    "    boost = min(math.log(access_count + 1) * 0.1, 0.5)\n",
    "    memory.metadata[\"importance\"] = min(\n",
    "        memory.metadata[\"importance\"] + boost, \n",
    "        1.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b460158",
   "metadata": {},
   "source": [
    "**最近性因素**\n",
    "- 強化當前上下文相關性\n",
    "- 提供短期動態權重以補足長期評分模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recency_boost(memory, half_life_days=7):\n",
    "    \"\"\"最近存取的記憶獲得加成\n",
    "    \n",
    "    Args:\n",
    "        half_life_days: 半衰期,預設 7 天後加成減半\n",
    "    \"\"\"\n",
    "    days_since_access = (now() - memory.metadata[\"recency\"]).days\n",
    "    \n",
    "    # 指數衰減模型\n",
    "    recency_factor = 2 ** (-days_since_access / half_life_days)\n",
    "    \n",
    "    # 最近 24 小時內存取的記憶獲得最高加成\n",
    "    if days_since_access == 0:\n",
    "        recency_factor = 1.5\n",
    "    \n",
    "    return recency_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0210759c",
   "metadata": {},
   "source": [
    "**混合衰減策略**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef58551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_decay(memory):\n",
    "    base_importance = memory.metadata[\"importance\"]\n",
    "    \n",
    "    # 時間因素\n",
    "    time_factor = time_decay(memory)\n",
    "    \n",
    "    # 頻率因素\n",
    "    freq_factor = frequency_boost(memory)\n",
    "    \n",
    "    # 最近性因素\n",
    "    recency_factor = recency_boost(memory)\n",
    "    \n",
    "    final_score = base_importance * time_factor * freq_factor * recency_factor\n",
    "    memory.metadata[\"importance\"] = min(final_score, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13a0ef7",
   "metadata": {},
   "source": [
    "### 定期清理低價值記憶\n",
    "**閾值策略**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112254d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_memories(memories, threshold=0.2):\n",
    "    \"\"\"刪除重要性低於閾值的記憶\"\"\"\n",
    "    return [m for m in memories if m.metadata[\"importance\"] >= threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3fde32",
   "metadata": {},
   "source": [
    "**批次清理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a1fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_cleanup(memory_store, batch_size=1000):\n",
    "    \"\"\"分批清理,避免阻塞\"\"\"\n",
    "    for i in range(0, len(memory_store), batch_size):\n",
    "        batch = memory_store[i:i+batch_size]\n",
    "        \n",
    "        # 更新衰減分數\n",
    "        for memory in batch:\n",
    "            hybrid_decay(memory)\n",
    "        \n",
    "        # 移除低分記憶\n",
    "        memory_store[i:i+batch_size] = cleanup_memories(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c0bc39",
   "metadata": {},
   "source": [
    "**防止重要記憶誤刪**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c927bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def protected_cleanup(memories, threshold=0.2):\n",
    "    protected_tags = [\"user_preference\", \"critical_error\", \"goal\"]\n",
    "    \n",
    "    cleaned = []\n",
    "    for m in memories:\n",
    "        # 保護標記的記憶\n",
    "        if any(tag in m.metadata[\"tags\"] for tag in protected_tags):\n",
    "            cleaned.append(m)\n",
    "        # 或重要性高於閾值\n",
    "        elif m.metadata[\"importance\"] >= threshold:\n",
    "            cleaned.append(m)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be04ec",
   "metadata": {},
   "source": [
    "### to be continued"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
